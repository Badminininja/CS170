Welcome to Joseph's Feature Selection Algorithm
Type in the name of the file to test: CS170_Small_Data__55.txt
Type the number of the algorithm you want to run.
    1) Forward Selection
    2) Backward Elimination
2
This dataset has 6 features (not including the class attribute), with 500 instances.
Running nearest neighbor with all 6 features, using "leaving-one-out" evaluation, I get an accuracy of: 90.6%
Beginning search.
    Using features(s) [2, 3, 4, 5, 6] accuracy is  92.2%
    Using features(s) [1, 3, 4, 5, 6] accuracy is  92.0%
    Using features(s) [1, 2, 4, 5, 6] accuracy is  93.2%
    Using features(s) [1, 2, 3, 5, 6] accuracy is  92.2%
    Using features(s) [1, 2, 3, 4, 6] accuracy is  87.0%
    Using features(s) [1, 2, 3, 4, 5] accuracy is  85.4%
Feature set [1, 2, 4, 5, 6] was best, accuracy is 93.2%
    Using features(s) [2, 4, 5, 6] accuracy is  93.6%
    Using features(s) [1, 4, 5, 6] accuracy is  96.0%
    Using features(s) [1, 2, 5, 6] accuracy is  95.6%
    Using features(s) [1, 2, 4, 6] accuracy is  90.0%
    Using features(s) [1, 2, 4, 5] accuracy is  87.8%
Feature set [1, 4, 5, 6] was best, accuracy is 96.0%
    Using features(s) [4, 5, 6] accuracy is  96.2%
    Using features(s) [1, 5, 6] accuracy is  97.8%
    Using features(s) [1, 4, 6] accuracy is  90.8%
    Using features(s) [1, 4, 5] accuracy is  86.8%
Feature set [1, 5, 6] was best, accuracy is 97.8%
    Using features(s) [5, 6] accuracy is  98.6%
    Using features(s) [1, 6] accuracy is  89.2%
    Using features(s) [1, 5] accuracy is  87.2%
Feature set [5, 6] was best, accuracy is 98.6%
    Using features(s) [6] accuracy is  92.2%
    Using features(s) [5] accuracy is  88.4%
Feature set [6] was best, accuracy is 92.2%
    Using features(s) [] accuracy is  80.0%
Feature set [] was best, accuracy is 80.0%

Finished search!! The best feature subset is [5, 6], which has an accuracy of 98.6%